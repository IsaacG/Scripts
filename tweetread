#!/bin/bash

#
# Read Twitter stuffs from a file.
# Twitter -> bitlbee -> irssi -> file -> this script for reading large amounts of lines
# Display lines one by one with a scroll up/down. Toggle save to keep for later. And xclip integration for visiting links
#
# Reads data from $in
# space/enter/j/n for next
# k/p for previous
# s toggles save (displays on quit for later use)
# q quits
# u to put URLs one by one into xclip
# l to refresh. Lazy way out of issue with long lines that wrap around...
#

#
# TODO
# Should have more robust handling around input file. And output... Save to input file with confirmation?
#

# On TERM, make cursor visible
trap 'tput cnorm' EXIT

file="${1:-./twitter}"
tmout="$TMOUT"
unset TMOUT

_size=$(tput lines)
_red=$(tput setaf 1)
_blue=$(tput setaf 4)
_magenta=$(tput setaf 5)
_cyan=$(tput setaf 6)
_def=$(tput setaf 9)

_cur_reset=$(tput clear;tput cup $((_size/3)) 1)
cur_reset () { printf "%s" "$_cur_reset" ;}

_init="$(tput civis; tput setaf 9)" # Make cursor invisible, reset color
init () { printf "%s" "$_init" ;}

# Print a new feed item over the old line
print () {
	cur_reset
	[[ ${save[index]} ]] && color="$_cyan" || color="$_def"
	printf "%s%d/%d %s%s%s\n" "$_red" $((index+1)) $max "$color" "${lines[index]}" "$_def"
	
	# Find all URLs and xclip them
	urls=""
	for word in ${lines[index]} ; do 
		! [[ $word = http* || $word = www* ]] && continue
		[[ $urls ]] && urls="$urls|$word" || urls="$word"
	done
	[[ $urls ]] && { 
		xclip -i <<< "$urls"
		printf "%s URL copied: %s%s\n" "$_magenta" "$urls" "$_def"
	}

	(( ${#save[@]} )) && printf "%d saved items" ${#save[@]}
}

reset_screen () {
	init
	print
}

if ! [[ -r $file ]] ; then
	echo "File $file not readable."
	exit 1
fi

lines=()
save=()
# The tweets to serve up
while read -r l ; do [[ $l ]] && lines+=("$l") ; done < "$file"
lines+=("${_red}No more items$_def")

index=0
max="${#lines[@]}"

# Starting point and first item
reset_screen

#shopt -s nocasematch
while read -sN1 ; do
	case "$REPLY" in
		"" | [nj] | " " ) # Next
			((index < max - 1 && index++))
			print
			;;
		[pk]) # Back/prev
			((index > 0 && index--))
			print
			;;
		[of]) # Open with ff
			firefox "$urls"
			((index < max - 1 && index++)) # Advance to next item on save
			print
			;;
		s) # Toggle save an item. Displayed on quit
			[[ ${save[index]} ]] && unset save[index] || save[index]=1
			((index < max - 1 && index++)) # Advance to next item on save
			print
			;;
		S) # Set everything as saved
			for ((i=0; i<= max; i++)) ; do save[i]=1 ; done
			print
			;;
		l) # Reset/refresh screen
			reset_screen
			;;
		q | ) # Quit, printing saved items
			printf "\n\n"
			if (( ${#save[@]} )) ; then 
				# Put together the saved entries
				out=()
				for i in "${!save[@]}" ; do out+=( "${lines[i]}" ) ; done
				# Write out saved entires to STDOUT or $file
				read -n1 -p "Write out ${#save[@]} saved entries to file $file? [y/N] "
				printf "\n"
				[[ $REPLY = [yY] ]] && printf "%s\n" "${out[@]}" > "$file" || printf "%s\n" "${out[@]}"
			else
				read -n1 -p "Clear file $file? [y/N] "
				printf "\n"
				[[ $REPLY = [yY] ]] && : > "$file"
			fi

			tput cnorm # Make cursor visible again
			TMOUT="$tmout"
			exit
			;;
		u ) # Move a URL to clip
			for word in ${lines[index]} ; do # Yay! Intentional word splitting
				! [[ $word = http* || $word = www* ]] && continue
				xclip -i <<< "$word"
				cur_reset
				printf "URL %s%s%s -> press any key to continue" "$_blue" "$word" "$_def"
				read -N1
			done
			print
			;;
	esac
done
		
